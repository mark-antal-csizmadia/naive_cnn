{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10, mnist\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_validation_set(x_train_all, y_train_all, x_validation_size):\n",
    "    \"\"\"\n",
    "    Createa a validation set from the training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train_all : np.array\n",
    "        The training data.\n",
    "    y_train_all : np.array\n",
    "        The training labels.\n",
    "    x_validation_size: int\n",
    "        The size of the validation dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x_train_raw: np.array\n",
    "        The new training data.\n",
    "    y_train_raw: np.array\n",
    "        The new training labels.\n",
    "    x_val_raw: np.array\n",
    "        The new validation data.\n",
    "    y_val_raw: np.array\n",
    "        The new validation labels.\n",
    "    \"\"\"\n",
    "    sss_val = \\\n",
    "        StratifiedShuffleSplit(\n",
    "        n_splits=1, \n",
    "        test_size=x_validation_size/x_train_all.shape[0],\n",
    "        random_state=110)\n",
    "    train_indices, validation_indices = list(sss_val.split(x_train_all,y_train_all))[0]\n",
    "    \n",
    "    x_train_raw = x_train_all[train_indices]\n",
    "    y_train_raw = y_train_all[train_indices]\n",
    "    x_val_raw = x_train_all[validation_indices]\n",
    "    y_val_raw = y_train_all[validation_indices]\n",
    "    \n",
    "    return x_train_raw, y_train_raw, x_val_raw, y_val_raw\n",
    "\n",
    "def reduce_data(data_x, data_y, keep_real_num):\n",
    "    \"\"\"\n",
    "    Since the CNN in the code is a naive implementation, it could take forever to properly train\n",
    "    on the entire datasets. As a result, they need to be reduced in size for faster training and inference.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_x : np.array\n",
    "        The data.\n",
    "    data_y : np.array\n",
    "        The labels.\n",
    "    keep_real_num: float\n",
    "        A real number in the interval (0.0,1.0]. Represents the amount of data to be kept in terms of percentage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_x_reduced : np.array\n",
    "        The reduced array of data.\n",
    "    data_y_reduced: np.array\n",
    "        The reduced array of labels.\n",
    "    \"\"\"\n",
    "    if keep_real_num != 1.0:\n",
    "        # Stratified shuffle.\n",
    "        sss = \\\n",
    "            StratifiedShuffleSplit(\n",
    "            n_splits=1, \n",
    "            test_size=(1-keep_real_num),\n",
    "            random_state=110)\n",
    "        keep_indices, drop_indices = list(sss.split(data_x,data_y))[0]\n",
    "        data_x_reduced = data_x[keep_indices]\n",
    "        data_y_reduced = data_y[keep_indices]\n",
    "    else:\n",
    "        data_x_reduced = data_x\n",
    "        data_y_reduced = data_y\n",
    "    \n",
    "    return data_x_reduced, data_y_reduced\n",
    "\n",
    "def verbose_classes_and_occurences(data_x, data_y):\n",
    "    # Check if the training and the validation data are balanced data sets (i.e.: have the same number of occurences per class). \n",
    "    y_unique, y_counts = np.unique(data_y, return_counts=True)\n",
    "\n",
    "    y_classes_and_occurences = dict(zip(y_unique, y_counts))\n",
    "    \n",
    "    return y_classes_and_occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shapes: x_train.shape=(11800, 28, 28, 1), y_train.shape=(11800,)\n",
      "validation shapes: x_val.shape=(200, 28, 28, 1), y_val.shape=(200,)\n",
      "test shapes: x_test.shape=(2000, 28, 28, 1), y_test.shape=(2000,)\n",
      "Training occurences: {0: 1165, 1: 1326, 2: 1172, 3: 1206, 4: 1149, 5: 1066, 6: 1164, 7: 1232, 8: 1150, 9: 1170}\n",
      "Validation occurences: {0: 20, 1: 22, 2: 20, 3: 20, 4: 19, 5: 18, 6: 20, 7: 21, 8: 20, 9: 20}\n",
      "Test occurences: {0: 196, 1: 227, 2: 206, 3: 202, 4: 196, 5: 178, 6: 192, 7: 206, 8: 195, 9: 202}\n"
     ]
    }
   ],
   "source": [
    "# Data preparation and preprocessing.\n",
    "\n",
    "# Load the MNIST hand-written digit dataset.\n",
    "(x_train_all, y_train_all), (x_test_all, y_test_all) = mnist.load_data()\n",
    "\n",
    "# Reshaping the array for feeding it into CNN.\n",
    "x_train_all = x_train_all.reshape(x_train_all.shape[0], 28, 28, 1)\n",
    "x_test_all = x_test_all.reshape(x_test_all.shape[0], 28, 28, 1)\n",
    "\n",
    "# Explicit casting to float to allow for double precision.\n",
    "x_train_all = x_train_all.astype('float32')\n",
    "x_test_all = x_test_all.astype('float32')\n",
    "\n",
    "# Normalizing the RGB codes by dividing it to the max RGB value. Here grayscale image, but still do it.\n",
    "x_train_all /= 255\n",
    "x_test_all /= 255\n",
    "\n",
    "# Validation dataset size.\n",
    "x_validation_size = 1000\n",
    "\n",
    "\n",
    "x_train_raw, y_train_raw, x_val_raw, y_val_raw = \\\n",
    "    separate_validation_set(x_train_all=x_train_all, y_train_all=y_train_all, \n",
    "                            x_validation_size=x_validation_size)\n",
    "\n",
    "keep_real_num = 0.2\n",
    "\n",
    "x_train, y_train = reduce_data(data_x=x_train_raw, data_y=y_train_raw, keep_real_num=keep_real_num)\n",
    "x_val, y_val = reduce_data(data_x=x_val_raw, data_y=y_val_raw, keep_real_num=keep_real_num)\n",
    "x_test, y_test = reduce_data(data_x=x_test_all, data_y=y_test_all, keep_real_num=keep_real_num)\n",
    "\n",
    "# Verbose.\n",
    "print('training shapes: x_train.shape={0}, y_train.shape={1}'.format(x_train.shape, y_train.shape))\n",
    "print('validation shapes: x_val.shape={0}, y_val.shape={1}'.format(x_val.shape, y_val.shape))\n",
    "print('test shapes: x_test.shape={0}, y_test.shape={1}'.format(x_test.shape, y_test.shape))\n",
    "\n",
    "print(\"Training occurences: {0}\".format(verbose_classes_and_occurences(data_x=x_train, data_y=y_train)))\n",
    "print(\"Validation occurences: {0}\".format(verbose_classes_and_occurences(data_x=x_val, data_y=y_val)))\n",
    "print(\"Test occurences: {0}\".format(verbose_classes_and_occurences(data_x=x_test, data_y=y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalLayer():\n",
    "    def __init__(self, n_C, n_H_prev, n_W_prev, n_C_prev, f=3, stride=1, pad=1):\n",
    "        self.n_C = n_C\n",
    "        self.n_H_prev = n_H_prev\n",
    "        self.n_W_prev = n_W_prev\n",
    "        self.n_C_prev = n_C_prev\n",
    "        self.f = f\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "        self.n_H = self.get_n_H()\n",
    "        self.n_W = self.get_n_W()\n",
    "        \n",
    "        self.W, self.W_memory, self.W_squared_sum_memory = self.initialize_W()\n",
    "        self.b, self.b_memory, self.b_squared_sum_memory = self.initialize_b()\n",
    "        \n",
    "        self.A_prev = None\n",
    "        \n",
    "    def get_n_H(self):\n",
    "        n_H = (self.n_H_prev - self.f + 2*self.pad) / self.stride + 1\n",
    "        if n_H % 1 != 0:\n",
    "            raise Exception(\"n_H is invalid, n_H={0}\\n\".format(n_H))\n",
    "        return int(n_H)\n",
    "    \n",
    "    def get_n_W(self):\n",
    "        n_W = (self.n_W_prev - self.f + 2*self.pad) / self.stride + 1\n",
    "        if n_W % 1 != 0:\n",
    "            raise Exception(\"n_W is invalid, n_W={0}\\n\".format(n_W))\n",
    "        return int(n_W)\n",
    "        \n",
    "    def initialize_W(self):\n",
    "        \"\"\"\n",
    "        W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        \"\"\"\n",
    "        W = np.random.normal(loc=0.0, scale=1.0, size=(self.f, self.f, self.n_C_prev, self.n_C)) * 10e-2\n",
    "        W_memory = np.zeros(W.shape)\n",
    "        W_squared_sum_memory = np.zeros(W.shape)\n",
    "        return W, W_memory, W_squared_sum_memory\n",
    "    \n",
    "    def initialize_b(self):\n",
    "        \"\"\"\n",
    "        b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        \"\"\"\n",
    "        b = np.zeros((1,1,1,self.n_C))\n",
    "        b_memory = np.zeros(b.shape)\n",
    "        b_squared_sum_memory = np.zeros(b.shape)\n",
    "        return b, b_memory, b_squared_sum_memory\n",
    "        \n",
    "\n",
    "    def zero_pad(self, X, pad):\n",
    "        \"\"\"\n",
    "        Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "        as illustrated in Figure 1.\n",
    "\n",
    "        Argument:\n",
    "        X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "        pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "        Returns:\n",
    "        X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ### (≈ 1 line)\n",
    "        X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return X_pad\n",
    "    \n",
    "    def conv_single_step(self, a_slice_prev, W, b):\n",
    "        \"\"\"\n",
    "        Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "        of the previous layer.\n",
    "\n",
    "        Arguments:\n",
    "        a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "        W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "        b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "        Returns:\n",
    "        Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n",
    "        s = np.multiply(a_slice_prev, W)\n",
    "        # Sum over all entries of the volume s.\n",
    "        Z = np.sum(s)\n",
    "        # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "        Z += float(b)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return Z\n",
    "    \n",
    "    def conv_forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- output activations of the previous layer, \n",
    "            numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "        hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "\n",
    "        Returns:\n",
    "        Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache of values needed for the conv_backward() function\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "        #(m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "        # Retrieve dimensions from W's shape (≈1 line)\n",
    "        #(f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "        # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "        #stride = hparameters[\"stride\"]\n",
    "        #pad = hparameters[\"pad\"]\n",
    "\n",
    "        # Compute the dimensions of the CONV output volume using the formula given above. \n",
    "        # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n",
    "        #n_H = int((n_H_prev - f + 2*pad) / stride + 1)\n",
    "        #n_W = int((n_W_prev - f + 2*pad) / stride + 1)\n",
    "        \n",
    "        m = A_prev.shape[0]\n",
    "        # Initialize the output volume Z with zeros. (≈1 line)\n",
    "        Z = np.zeros((m, self.n_H, self.n_W, self.n_C))\n",
    "\n",
    "        # Create A_prev_pad by padding A_prev\n",
    "        A_prev_pad = self.zero_pad(A_prev, self.pad)\n",
    "\n",
    "        for i in range(m):               # loop over the batch of training examples\n",
    "            a_prev_pad = A_prev_pad[i]               # Select ith training example's padded activation\n",
    "            for h in range(self.n_H):           # loop over vertical axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                vert_start = h * self.stride\n",
    "                vert_end = vert_start + self.f\n",
    "\n",
    "                for w in range(self.n_W):       # loop over horizontal axis of the output volume\n",
    "                    # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n",
    "                    horiz_start = w * self.stride\n",
    "                    horiz_end = horiz_start + self.f\n",
    "\n",
    "                    for c in range(self.n_C):   # loop over channels (= #filters) of the output volume\n",
    "\n",
    "                        # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                        # f,f,n_C_prev\n",
    "                        a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:]\n",
    "\n",
    "                        # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n",
    "                        weights = self.W[:,:,:,c]\n",
    "                        biases = self.b[:,:,:,c]\n",
    "                        Z[i, h, w, c] = self.conv_single_step(a_slice_prev=a_slice_prev, W=weights, b=biases)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert(Z.shape == (m, self.n_H, self.n_W, self.n_C))\n",
    "\n",
    "        # Save information in \"cache\" for the backprop\n",
    "        #cache = (A_prev, W, b, hparameters)\n",
    "        self.A_prev = A_prev\n",
    "\n",
    "        return Z\n",
    "    \n",
    "    def conv_backward(self, dZ, regularization_rate):\n",
    "        \"\"\"\n",
    "        Implement the backward propagation for a convolution function\n",
    "\n",
    "        Arguments:\n",
    "        dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                   numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "              numpy array of shape (f, f, n_C_prev, n_C)\n",
    "        db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "              numpy array of shape (1, 1, 1, n_C)\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Retrieve information from \"cache\"\n",
    "        #(A_prev, W, b, hparameters) = cache\n",
    "        A_prev = self.A_prev\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        # Retrieve dimensions from A_prev's shape\n",
    "        #(m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "        # Retrieve dimensions from W's shape\n",
    "        #(f, f, n_C_prev, n_C) = W.shape\n",
    "\n",
    "        # Retrieve information from \"hparameters\"\n",
    "        #stride = hparameters[\"stride\"]\n",
    "        #pad = hparameters[\"pad\"]\n",
    "\n",
    "        # Retrieve dimensions from dZ's shape\n",
    "        #(m, n_H, n_W, n_C) = dZ.shape\n",
    "\n",
    "        # Initialize dA_prev, dW, db with the correct shapes\n",
    "        dA_prev = np.zeros((m, self.n_H_prev, self.n_W_prev, self.n_C_prev))                           \n",
    "        dW = np.zeros((self.f, self.f, self.n_C_prev, self.n_C))\n",
    "        db = np.zeros((1, 1, 1, self.n_C))\n",
    "\n",
    "        # Pad A_prev and dA_prev\n",
    "        A_prev_pad = self.zero_pad(X=A_prev, pad=self.pad)\n",
    "        dA_prev_pad = self.zero_pad(X=dA_prev, pad=self.pad)\n",
    "\n",
    "        for i in range(m):                       # loop over the training examples\n",
    "\n",
    "            # select ith training example from A_prev_pad and dA_prev_pad\n",
    "            a_prev_pad = A_prev_pad[i]\n",
    "            da_prev_pad = dA_prev_pad[i]\n",
    "\n",
    "            for h in range(self.n_H):                   # loop over vertical axis of the output volume\n",
    "                for w in range(self.n_W):               # loop over horizontal axis of the output volume\n",
    "                    for c in range(self.n_C):           # loop over the channels of the output volume\n",
    "\n",
    "                        # Find the corners of the current \"slice\"\n",
    "                        vert_start = self.stride * h\n",
    "                        vert_end = vert_start + self.f\n",
    "                        horiz_start = self.stride * w\n",
    "                        horiz_end = horiz_start + self.f\n",
    "\n",
    "                        # Use the corners to define the slice from a_prev_pad\n",
    "                        a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                        # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                        da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += self.W[:,:,:,c] * dZ[i,h,w,c]\n",
    "                        dW[:,:,:,c] += a_slice * dZ[i,h,w,c]\n",
    "                        db[:,:,:,c] += dZ[i,h,w,c]\n",
    "\n",
    "            # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "            if self.pad != 0:\n",
    "                dA_prev[i, :, :, :] = da_prev_pad[self.pad:-self.pad, self.pad:-self.pad, :]\n",
    "            else:\n",
    "                dA_prev[i, :, :, :] = da_prev_pad\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert(dA_prev.shape == (m, self.n_H_prev, self.n_W_prev, self.n_C_prev))\n",
    "\n",
    "        return dA_prev, dW, db\n",
    "    \n",
    "    def update_parameters(self, dW, db, learning_rate, adam_beta_1, adam_beta_2, adam_iterator_counter):\n",
    "        #print(\"W before\")\n",
    "        #print(self.W)\n",
    "        #self.W = self.W - learning_rate * dW\n",
    "        self.W_memory = adam_beta_1 * self.W_memory + (1 - adam_beta_1) * dW\n",
    "        W_memory_bias_correction = self.W_memory / (1 - adam_beta_1**adam_iterator_counter)\n",
    "        self.W_squared_sum_memory = \\\n",
    "            adam_beta_2 * self.W_squared_sum_memory + (1 - adam_beta_2) * np.power(dW,2)\n",
    "        W_squared_sum_memory_bias_correction = self.W_squared_sum_memory / (1 - adam_beta_2**adam_iterator_counter)\n",
    "        self.W = \\\n",
    "            self.W - learning_rate * W_memory_bias_correction / (np.sqrt(W_squared_sum_memory_bias_correction) + 1e-8)\n",
    "        #print(\"W after\")\n",
    "        #print(self.W)\n",
    "        \n",
    "        #print(\"b before\")\n",
    "        #print(self.b)\n",
    "        #self.b = self.b - learning_rate * db\n",
    "        self.b_memory = adam_beta_1 * self.b_memory + (1 - adam_beta_1) * db\n",
    "        b_memory_bias_correction = self.b_memory  / (1 - adam_beta_1**adam_iterator_counter)\n",
    "        self.b_squared_sum_memory = \\\n",
    "            adam_beta_2 * self.b_squared_sum_memory + (1 - adam_beta_2) * np.power(db,2)\n",
    "        b_squared_sum_memory_bias_correction = self.b_squared_sum_memory / (1 - adam_beta_2**adam_iterator_counter)\n",
    "        self.b = \\\n",
    "            self.b - learning_rate * b_memory_bias_correction / (np.sqrt(b_squared_sum_memory_bias_correction) + 1e-8)\n",
    "        #print(\"b after\")\n",
    "        #print(self.b)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr_text = \"CONV: n_C:{0},n_H_prev:{1},n_W_prev:{2},n_C_prev:{3},f:{4},stride:{5},pad:{6},n_H:{7},n_W:{8}\".format(\n",
    "            self.n_C, self.n_H_prev, self.n_W_prev, self.n_C_prev, self.f, self.stride, self.pad,\n",
    "            self.n_H, self.n_W)\n",
    "        return repr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(ConvolutionalLayer):\n",
    "    def __init__(self, n_C, n_H_prev, n_W_prev, n_C_prev, f=None, stride=1, pad=0):\n",
    "        assert(n_H_prev == n_W_prev)\n",
    "        f = n_H_prev\n",
    "        super().__init__(n_C, n_H_prev, n_W_prev, n_C_prev, f, stride, pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLayer():\n",
    "    def __init__(self, n_H_prev, n_W_prev, n_C_prev, f, stride, mode=\"max\"):\n",
    "        self.n_H_prev = n_H_prev\n",
    "        self.n_W_prev = n_W_prev\n",
    "        self.n_C_prev = n_C_prev\n",
    "        self.f = f\n",
    "        self.stride = stride\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.n_H = self.get_n_H()\n",
    "        self.n_W = self.get_n_W()\n",
    "        self.n_C = self.n_C_prev\n",
    "        \n",
    "        self.A_prev = None\n",
    "        \n",
    "    def get_n_H(self):\n",
    "        n_H = (self.n_H_prev - self.f) / self.stride + 1\n",
    "        if n_H % 1 != 0:\n",
    "            raise Exception(\"n_H is invalid, n_H={0}\\n\".format(n_H))\n",
    "        return int(n_H)\n",
    "    \n",
    "    def get_n_W(self):\n",
    "        n_W = (self.n_W_prev - self.f) / self.stride + 1\n",
    "        if n_W % 1 != 0:\n",
    "            raise Exception(\"n_W is invalid, n_W={0}\\n\".format(n_W))\n",
    "        return int(n_W)\n",
    "    \n",
    "    def pool_forward(self, A_prev, mode = \"max\"):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the pooling layer\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "        hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "        mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "\n",
    "        Returns:\n",
    "        A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "        cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve dimensions from the input shape\n",
    "        #(m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "        # Retrieve hyperparameters from \"hparameters\"\n",
    "        #f = hparameters[\"f\"]\n",
    "        #stride = hparameters[\"stride\"]\n",
    "\n",
    "        # Define the dimensions of the output\n",
    "        #n_H = int(1 + (n_H_prev - f) / stride)\n",
    "        #n_W = int(1 + (n_W_prev - f) / stride)\n",
    "        #n_C = n_C_prev\n",
    "\n",
    "        m = A_prev.shape[0]\n",
    "        # Initialize output matrix A\n",
    "        A = np.zeros((m, self.n_H, self.n_W, self.n_C))              \n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(m):                         # loop over the training examples\n",
    "            for h in range(self.n_H):                     # loop on the vertical axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                vert_start = self.stride * h\n",
    "                vert_end = vert_start + self.f\n",
    "\n",
    "                for w in range(self.n_W):                 # loop on the horizontal axis of the output volume\n",
    "                    # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                    horiz_start = self.stride * w\n",
    "                    horiz_end = horiz_start + self.f\n",
    "\n",
    "                    for c in range (self.n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                        # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                        a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                        # Compute the pooling operation on the slice. \n",
    "                        # Use an if statement to differentiate the modes. \n",
    "                        # Use np.max and np.mean.\n",
    "                        if self.mode == \"max\":\n",
    "                            A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                        elif self.mode == \"average\":\n",
    "                            A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "                        else:\n",
    "                            raise Exception(\"Invalid mode of pooling\\n\")\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "        #cache = (A_prev, hparameters)\n",
    "        self.A_prev = A_prev\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert(A.shape == (m, self.n_H, self.n_W, self.n_C))\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def create_mask_from_window(self, x):\n",
    "        \"\"\"\n",
    "        Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "\n",
    "        Arguments:\n",
    "        x -- Array of shape (f, f)\n",
    "\n",
    "        Returns:\n",
    "        mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ### (≈1 line)\n",
    "        mask = (x == x.max())\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return mask\n",
    "    \n",
    "    def distribute_value(self, dz, shape):\n",
    "        \"\"\"\n",
    "        Distributes the input value in the matrix of dimension shape\n",
    "\n",
    "        Arguments:\n",
    "        dz -- input scalar\n",
    "        shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "\n",
    "        Returns:\n",
    "        a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # Retrieve dimensions from shape (≈1 line)\n",
    "        (n_H, n_W) = shape\n",
    "\n",
    "        # Compute the value to distribute on the matrix (≈1 line)\n",
    "        average = dz/(n_H*n_W)\n",
    "\n",
    "        # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "        a = np.ones((n_H, n_W)) * average\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def pool_backward(self, dA, mode = \"max\"):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the pooling layer\n",
    "\n",
    "        Arguments:\n",
    "        dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "        cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "        mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Retrieve information from cache (≈1 line)\n",
    "        #(A_prev, hparameters) = cache\n",
    "        A_prev = self.A_prev\n",
    "\n",
    "        # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "        #stride = hparameters[\"stride\"]\n",
    "        #f = hparameters[\"f\"]\n",
    "\n",
    "        # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "        #m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        #m, n_H, n_W, n_C = dA.shape\n",
    "        \n",
    "        m = A_prev.shape[0]\n",
    "        # Initialize dA_prev with zeros (≈1 line)\n",
    "        dA_prev = np.zeros(A_prev.shape)\n",
    "\n",
    "        for i in range(m):                       # loop over the training examples\n",
    "\n",
    "            # select training example from A_prev (≈1 line)\n",
    "            a_prev = A_prev[i]\n",
    "\n",
    "            for h in range(self.n_H):                   # loop on the vertical axis\n",
    "                for w in range(self.n_W):               # loop on the horizontal axis\n",
    "                    for c in range(self.n_C):           # loop over the channels (depth)\n",
    "\n",
    "                        # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                        vert_start = self.stride * h\n",
    "                        vert_end = vert_start + self.f\n",
    "                        horiz_start = self.stride * w\n",
    "                        horiz_end = horiz_start + self.f\n",
    "\n",
    "                        # Compute the backward propagation in both modes.\n",
    "                        if self.mode == \"max\":\n",
    "\n",
    "                            # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                            a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                            # Create the mask from a_prev_slice (≈1 line)\n",
    "                            mask = self.create_mask_from_window(a_prev_slice)\n",
    "                            # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                            dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += \\\n",
    "                                np.multiply(mask, dA[i, h, w, c])\n",
    "\n",
    "                        elif self.mode == \"average\":\n",
    "\n",
    "                            # Get the value a from dA (≈1 line)\n",
    "                            da = dA[i, h, w, c]\n",
    "                            # Define the shape of the filter as fxf (≈1 line)\n",
    "                            shape = (self.f, self.f)\n",
    "                            # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                            dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += \\\n",
    "                                self.distribute_value(da, shape=shape)\n",
    "                        \n",
    "                        else:\n",
    "                            raise Exception(\"Invalid mode of pooling\\n\")\n",
    "\n",
    "        ### END CODE ###\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert(dA_prev.shape == A_prev.shape)\n",
    "\n",
    "        return dA_prev\n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr_text = \"POOL: n_H_prev:{0},n_W_prev:{1},n_C_prev:{2},f:{3},stride:{4},n_H:{5},n_W:{6},n_C:{7}\".format(\n",
    "            self.n_H_prev, self.n_W_prev, self.n_C_prev, self.f, self.stride, self.n_H, self.n_W, self.n_C)\n",
    "        return repr_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLuLayer():\n",
    "    alpha = 0\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.Z_prev = None\n",
    "    \n",
    "    def relu_forward(self, Z_prev):\n",
    "        self.Z_prev = Z_prev\n",
    "        relu_activation = np.maximum(0,Z_prev)\n",
    "        assert relu_activation.shape == Z_prev.shape\n",
    "        \n",
    "        return relu_activation\n",
    "    \n",
    "    def relu_backward(self, dA):\n",
    "        relu_grad = (self.Z_prev > 0) * dA\n",
    "        assert relu_grad.shape == self.Z_prev.shape\n",
    "        return relu_grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"RELU LAYER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidLayer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def sigmoid_forward(self, Z_prev):\n",
    "        self.Z_prev = Z_prev\n",
    "        self.sigmoid_activation = 1 / (1 + np.exp(-Z_prev))\n",
    "        return self.sigmoid_activation\n",
    "    \n",
    "    def sigmoid_backward(self, dA):\n",
    "        sigmoid_grad = np.multiply(self.sigmoid_activation, (1-self.sigmoid_activation))\n",
    "        return sigmoid_grad\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"SIGMOID LAYER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork():\n",
    "    learning_rate = 0.001\n",
    "    regularization_rate = 0.5\n",
    "    adam_beta_1 = 0.9\n",
    "    adam_beta_2 = 0.99\n",
    "    adam_iterator_counter = 1\n",
    "    \n",
    "    def __init__(self, architecture, compute_validation):\n",
    "        self.compute_validation = compute_validation\n",
    "        \n",
    "        self.layers = []\n",
    "        for layer_idx, layer_info in enumerate(architecture):\n",
    "            if layer_info[0] == \"CONV\":\n",
    "                layer = \\\n",
    "                    ConvolutionalLayer(n_C=layer_info[1], \n",
    "                                       n_H_prev=layer_info[2], \n",
    "                                       n_W_prev=layer_info[3], \n",
    "                                       n_C_prev=layer_info[4], \n",
    "                                       f=layer_info[5], \n",
    "                                       stride=layer_info[6], \n",
    "                                       pad=layer_info[7])\n",
    "            elif layer_info[0] == \"FC\":\n",
    "                layer = \\\n",
    "                    FullyConnectedLayer(n_C=layer_info[1], \n",
    "                                        n_H_prev=layer_info[2], \n",
    "                                        n_W_prev=layer_info[3], \n",
    "                                        n_C_prev=layer_info[4])\n",
    "            elif layer_info[0] == \"POOL\":\n",
    "                layer = \\\n",
    "                    PoolingLayer(n_H_prev=layer_info[1],\n",
    "                                n_W_prev=layer_info[2],\n",
    "                                n_C_prev=layer_info[3],\n",
    "                                f=layer_info[4],\n",
    "                                stride=layer_info[5],\n",
    "                                mode=layer_info[6])\n",
    "            elif layer_info[0] == \"RELU\":\n",
    "                layer = ReLuLayer()\n",
    "            elif layer_info[0] == \"SIGMOID\":\n",
    "                layer = SigmoidLayer()\n",
    "            else:\n",
    "                raise Exception(\"Invalid layer type {0}\".format(layer_info[0]))\n",
    "            self.layers.append(layer)\n",
    "    \n",
    "    def forward_prop(self, X):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ConvolutionalLayer):\n",
    "                print(\"forward propagating={0}\".format(layer))\n",
    "                X = layer.conv_forward(X)\n",
    "            elif isinstance(layer, PoolingLayer):\n",
    "                print(\"forward propagating={0}\".format(layer))\n",
    "                X = layer.pool_forward(X)\n",
    "            elif isinstance(layer, ReLuLayer):\n",
    "                print(\"forward propagating={0}\".format(layer))\n",
    "                X = layer.relu_forward(X)\n",
    "            elif isinstance(layer, SigmoidLayer):\n",
    "                print(\"forward propagating={0}\".format(layer))\n",
    "                X = layer.sigmoid_forward(X)\n",
    "            else:\n",
    "                raise Exception(\"Invalid layer\\n\")\n",
    "        return X\n",
    "    \n",
    "    def back_prop(self, dA):\n",
    "        for layer in reversed(self.layers):\n",
    "            if isinstance(layer, ConvolutionalLayer):\n",
    "                print(\"backpropagating={0}\".format(layer))\n",
    "                dA, dW, db = layer.conv_backward(dZ=dA, regularization_rate=self.regularization_rate)\n",
    "                layer.update_parameters(dW=dW, db=db, learning_rate=self.learning_rate, \n",
    "                                        adam_beta_1=self.adam_beta_1, adam_beta_2=self.adam_beta_2,\n",
    "                                        adam_iterator_counter=self.adam_iterator_counter)\n",
    "            elif isinstance(layer, PoolingLayer):\n",
    "                print(\"backpropagating={0}\".format(layer))\n",
    "                dA = layer.pool_backward(dA)\n",
    "            elif isinstance(layer, ReLuLayer):\n",
    "                print(\"backpropagating={0}\".format(layer))\n",
    "                \"\"\" HEYY WTF THIS IS NOT THE CORRECT BACKPROP, DA IS NOT DZ\"\"\"\n",
    "                dA = layer.relu_backward(dA)\n",
    "            elif isinstance(layer, SigmoidLayer):\n",
    "                print(\"backpropagating={0}\".format(layer))\n",
    "                \"\"\" HEYY WTF THIS IS NOT THE CORRECT BACKPROP, DA IS NOT DZ\"\"\"\n",
    "                dA = layer.sigmoid_backward(dA)\n",
    "            else:\n",
    "                raise Exception(\"Invalid layer\\n\")\n",
    "        self.adam_iterator_counter = self.adam_iterator_counter + 1\n",
    "    \n",
    "    def softmax_classifier_forward(self, X):\n",
    "        m = X.shape[0]\n",
    "        X = X.reshape(m,-1)\n",
    "        #Z = Z.reshape(m,-1)\n",
    "        #Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "        #scores = Z\n",
    "        #exp_scores = np.exp(scores - np.max(scores, axis=1, keepdims=True))\n",
    "        #probs = exp_scores/np.sum(exp_scores,axis=1,keepdims=True)\n",
    "        #exp_scores = np.exp(Z)\n",
    "        #exp_scores_normalizer = np.sum(np.exp(Z), axis=1, keepdims=True)\n",
    "        #probabilities = exp_scores/exp_scores_normalizer\n",
    "        #probabilities = probs\n",
    "        \n",
    "        \n",
    "        shifted_logits = X - np.max(X, axis=1, keepdims=True)\n",
    "        Z = np.sum(np.exp(shifted_logits), axis=1, keepdims=True)\n",
    "        log_probs = shifted_logits - np.log(Z)\n",
    "        probs = np.exp(log_probs)\n",
    "        \n",
    "        #print(\"probs\")\n",
    "        #print(probs)\n",
    "        \n",
    "        assert np.sum(probs, axis=1, keepdims=True).all() == 1.0\n",
    "        return probs, log_probs\n",
    "    \n",
    "    def softmax_classifier_backward(self, probs, y):\n",
    "        m = probs.shape[0]\n",
    "        c = probs.shape[1]\n",
    "        y = y.reshape(m,)\n",
    "        #dscores = probabilities\n",
    "        #dscores[np.arange(m),y] -= 1\n",
    "        #dscores /= m\n",
    "        \n",
    "        probs[np.arange(m),y] -= 1\n",
    "        \n",
    "        #print(\"y\")\n",
    "        #print(y)\n",
    "        \n",
    "        \n",
    "        #print(\"grad\")\n",
    "        #print(probs)\n",
    "        \n",
    "        probs /= m\n",
    "        \n",
    "        return probs.reshape(m, 1, 1, c)\n",
    "    \n",
    "    def full_backprop(self, probs, y):\n",
    "        dZ = self.softmax_classifier_backward(probs=probs, y=y)\n",
    "        self.back_prop(dA=dZ)\n",
    "    \n",
    "    def compute_cross_entropy(self, log_probs, y, regularization_rate):\n",
    "        m = log_probs.shape[0]\n",
    "        y = y.reshape(m,)\n",
    "        #correct_logprobs = -np.log(probabilities[np.arange(m),y])\n",
    "        #print(\"correct_logprobs\")\n",
    "        #print(correct_logprobs)\n",
    "        #data_loss = np.sum(correct_logprobs)/m\n",
    "        #loss = data_loss\n",
    "        \n",
    "        #print(\"log_probs\")\n",
    "        #print(log_probs)\n",
    "        #print(log_probs.shape)\n",
    "        #print(log_probs[np.arange(m),y])\n",
    "        #print(log_probs[np.arange(m),y].shape)\n",
    "        \n",
    "        loss = -np.sum(log_probs[np.arange(m),y]) / m\n",
    "        \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict(self, probabilities, y):\n",
    "        m = probabilities.shape[0]\n",
    "        y = y.reshape(m,)\n",
    "        predicitons = np.argmax(probabilities, axis=1)\n",
    "        accuracy = accuracy_score(y, predicitons)\n",
    "        return predicitons, accuracy\n",
    "    \n",
    "    def inference(self, X, y):\n",
    "        Z = self.forward_prop(X=X)\n",
    "        probs, log_probs = self.softmax_classifier_forward(X=Z)\n",
    "        loss = self.compute_cross_entropy(log_probs=log_probs, y=y,\n",
    "                                          regularization_rate=self.regularization_rate)\n",
    "        predictions, accuracy = self.predict(probabilities=probs, y=y)\n",
    "        return probs, loss, predictions, accuracy\n",
    "    \n",
    "    def train(self, X, y, epoch, minibatch_size, X_val, y_val):\n",
    "        \n",
    "        training_loss = []\n",
    "        training_accuracy = []\n",
    "        \n",
    "        validation_loss = []\n",
    "        validation_accuracy = []\n",
    "        \n",
    "        for epoch_n in range(epoch):\n",
    "            \n",
    "            print(\"EPOCH:{0} STARTED\\n\".format(epoch_n+1))\n",
    "            \n",
    "            minibatch_iterator = range(0, X.shape[0], minibatch_size)\n",
    "            \n",
    "            for minibatch_idx, i in enumerate(minibatch_iterator):\n",
    "                \n",
    "                print(\"EPOCH:{0}, MINIBATCH_ID:{1}/{2} STARTED\\n\".format(\n",
    "                    epoch_n+1, minibatch_idx+1, len(list(minibatch_iterator))))\n",
    "                \n",
    "                X_minibatch = X[i:i+minibatch_size]\n",
    "                y_minibatch = y[i:i+minibatch_size]\n",
    "                \n",
    "                print(\"\\ttraining inference...\")\n",
    "                \n",
    "                probs, loss, predictions, accuracy = self.inference(X=X_minibatch, y=y_minibatch)\n",
    "                \n",
    "                training_loss.append(loss)\n",
    "                training_accuracy.append(accuracy)\n",
    "                \n",
    "                print(\"\\ttraining backpropagation...\")\n",
    "                \n",
    "                self.full_backprop(probs=probs, y=y_minibatch)\n",
    "                \n",
    "                if self.compute_validation:\n",
    "                    print(\"\\tvalidation inference...\")\n",
    "                \n",
    "                    probs_val, loss_val, predictions_val, accuracy_val = self.inference(X=X_val, y=y_val)\n",
    "                \n",
    "                    validation_loss.append(loss_val)\n",
    "                    validation_accuracy.append(accuracy_val)\n",
    "                    \n",
    "                    validation_info = \"validation loss={0:.4f}, validation accuracy={1:.4f}%\".format(loss_val, accuracy_val*100)\n",
    "                \n",
    "                print(\"\\nEPOCH:{0}, MINIBATCH_ID:{1}/{2} RESULTS\".format(\n",
    "                    epoch_n+1, minibatch_idx+1, len(list(minibatch_iterator))))\n",
    "                training_info = \"training loss={0:.4f}, training accuracy={1:.4f}%\".format(loss, accuracy*100)\n",
    "                #validation_info = \"validation loss={0:.4f}, validation accuracy={1:.4f}%\".format(loss_val, accuracy_val*100)\n",
    "                info = training_info + \"\\n\" + validation_info if self.compute_validation else training_info\n",
    "                print(\"{0}\".format(info))\n",
    "                \n",
    "            print(\"\\nEPOCH:{0} FINISHED\\n\".format(epoch_n+1))\n",
    "            print(\"_\"*10)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        \n",
    "        fig, ax = plt.subplots(4,1, figsize=(12,15))\n",
    "        \n",
    "        left  = 0.125  # the left side of the subplots of the figure\n",
    "        right = 0.9    # the right side of the subplots of the figure\n",
    "        bottom = 0.1   # the bottom of the subplots of the figure\n",
    "        top = 0.9      # the top of the subplots of the figure\n",
    "        wspace = 0.2   # the amount of width reserved for blank space between subplots\n",
    "        hspace = 2.2   # the amount of height reserved for white space between subplots\n",
    "        \n",
    "        plt.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace)\n",
    "        \n",
    "        ax[0].plot(range(len(training_loss)), training_loss)\n",
    "        ax[0].set_xlabel(\"Minibatch runs\")\n",
    "        ax[0].set_ylabel(\"Loss\")\n",
    "        ax[0].set_title(\"Training loss\")\n",
    "                      \n",
    "        ax[1].plot(range(len(training_accuracy)), np.array(training_accuracy)*100)\n",
    "        ax[1].set_xlabel(\"Minibatch runs\")\n",
    "        ax[1].set_ylabel(\"Accuracy [%]\")\n",
    "        ax[1].set_title(\"Training accuracy\")\n",
    "        \n",
    "        ax[2].plot(range(len(validation_loss)), validation_loss)\n",
    "        ax[2].set_xlabel(\"Minibatch runs\")\n",
    "        ax[2].set_ylabel(\"Loss\")\n",
    "        ax[2].set_title(\"Validation loss\")\n",
    "                      \n",
    "        ax[3].plot(range(len(validation_accuracy)), np.array(validation_accuracy)*100)\n",
    "        ax[3].set_xlabel(\"Minibatch runs\")\n",
    "        ax[3].set_ylabel(\"Accuracy [%]\")\n",
    "        ax[3].set_title(\"Validation accuracy\")\n",
    "        \n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr_text = []\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            repr_text.append(\"Index={0}, layer={1}\".format(layer_idx, layer))\n",
    "            repr_text.append(\"\\n\")\n",
    "        return \"\".join(repr_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random architecture.\n",
    "\n",
    "# CONV: n_C, n_H_prev, n_W_prev, n_C_prev, f, stride, pad\n",
    "# CONV: (n_H_prev - f + 2*pad) / stride + 1\n",
    "# POOL: n_H_prev, n_W_prev, n_C_prev, f, stride, mode=\"max\" or \"average\"\n",
    "\n",
    "l0 = (\"CONV\", 16, 28, 28, 1, 3, 1, 1)\n",
    "l0_relu = (\"RELU\",)\n",
    "l1 = (\"POOL\", 28, 28, 16, 2, 2, \"max\")\n",
    "l2 = (\"FC\", 16, 14, 14, 16)\n",
    "l2_relu = (\"RELU\",)\n",
    "l3 = (\"FC\", 10, 1, 1, 16)\n",
    "\n",
    "architecture=[l0, l0_relu, l1, l2, l2_relu, l3]\n",
    "\n",
    "#CNN = ConvolutionalNeuralNetwork(architecture=architecture)\n",
    "#print(CNN)\n",
    "#CNN.train(X=x_train_1, y=y_train_1, epoch=5, minibatch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index=0, layer=CONV: n_C:6,n_H_prev:28,n_W_prev:28,n_C_prev:1,f:5,stride:1,pad:0,n_H:24,n_W:24\n",
      "Index=1, layer=RELU LAYER\n",
      "Index=2, layer=POOL: n_H_prev:24,n_W_prev:24,n_C_prev:6,f:2,stride:2,n_H:12,n_W:12,n_C:6\n",
      "Index=3, layer=CONV: n_C:16,n_H_prev:12,n_W_prev:12,n_C_prev:6,f:5,stride:1,pad:0,n_H:8,n_W:8\n",
      "Index=4, layer=RELU LAYER\n",
      "Index=5, layer=POOL: n_H_prev:8,n_W_prev:8,n_C_prev:16,f:2,stride:2,n_H:4,n_W:4,n_C:16\n",
      "Index=6, layer=CONV: n_C:120,n_H_prev:4,n_W_prev:4,n_C_prev:16,f:4,stride:1,pad:0,n_H:1,n_W:1\n",
      "Index=7, layer=RELU LAYER\n",
      "Index=8, layer=CONV: n_C:84,n_H_prev:1,n_W_prev:1,n_C_prev:120,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "Index=9, layer=RELU LAYER\n",
      "Index=10, layer=CONV: n_C:10,n_H_prev:1,n_W_prev:1,n_C_prev:84,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "\n",
      "EPOCH:1 STARTED\n",
      "\n",
      "EPOCH:1, MINIBATCH_ID:1/369 STARTED\n",
      "\n",
      "\ttraining inference...\n",
      "forward propagating=CONV: n_C:6,n_H_prev:28,n_W_prev:28,n_C_prev:1,f:5,stride:1,pad:0,n_H:24,n_W:24\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=POOL: n_H_prev:24,n_W_prev:24,n_C_prev:6,f:2,stride:2,n_H:12,n_W:12,n_C:6\n",
      "forward propagating=CONV: n_C:16,n_H_prev:12,n_W_prev:12,n_C_prev:6,f:5,stride:1,pad:0,n_H:8,n_W:8\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=POOL: n_H_prev:8,n_W_prev:8,n_C_prev:16,f:2,stride:2,n_H:4,n_W:4,n_C:16\n",
      "forward propagating=CONV: n_C:120,n_H_prev:4,n_W_prev:4,n_C_prev:16,f:4,stride:1,pad:0,n_H:1,n_W:1\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=CONV: n_C:84,n_H_prev:1,n_W_prev:1,n_C_prev:120,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=CONV: n_C:10,n_H_prev:1,n_W_prev:1,n_C_prev:84,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "\ttraining backpropagation...\n",
      "backpropagating=CONV: n_C:10,n_H_prev:1,n_W_prev:1,n_C_prev:84,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "backpropagating=RELU LAYER\n",
      "backpropagating=CONV: n_C:84,n_H_prev:1,n_W_prev:1,n_C_prev:120,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "backpropagating=RELU LAYER\n",
      "backpropagating=CONV: n_C:120,n_H_prev:4,n_W_prev:4,n_C_prev:16,f:4,stride:1,pad:0,n_H:1,n_W:1\n",
      "backpropagating=POOL: n_H_prev:8,n_W_prev:8,n_C_prev:16,f:2,stride:2,n_H:4,n_W:4,n_C:16\n",
      "backpropagating=RELU LAYER\n",
      "backpropagating=CONV: n_C:16,n_H_prev:12,n_W_prev:12,n_C_prev:6,f:5,stride:1,pad:0,n_H:8,n_W:8\n",
      "backpropagating=POOL: n_H_prev:24,n_W_prev:24,n_C_prev:6,f:2,stride:2,n_H:12,n_W:12,n_C:6\n",
      "backpropagating=RELU LAYER\n",
      "backpropagating=CONV: n_C:6,n_H_prev:28,n_W_prev:28,n_C_prev:1,f:5,stride:1,pad:0,n_H:24,n_W:24\n",
      "\tvalidation inference...\n",
      "forward propagating=CONV: n_C:6,n_H_prev:28,n_W_prev:28,n_C_prev:1,f:5,stride:1,pad:0,n_H:24,n_W:24\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=POOL: n_H_prev:24,n_W_prev:24,n_C_prev:6,f:2,stride:2,n_H:12,n_W:12,n_C:6\n",
      "forward propagating=CONV: n_C:16,n_H_prev:12,n_W_prev:12,n_C_prev:6,f:5,stride:1,pad:0,n_H:8,n_W:8\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=POOL: n_H_prev:8,n_W_prev:8,n_C_prev:16,f:2,stride:2,n_H:4,n_W:4,n_C:16\n",
      "forward propagating=CONV: n_C:120,n_H_prev:4,n_W_prev:4,n_C_prev:16,f:4,stride:1,pad:0,n_H:1,n_W:1\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=CONV: n_C:84,n_H_prev:1,n_W_prev:1,n_C_prev:120,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=CONV: n_C:10,n_H_prev:1,n_W_prev:1,n_C_prev:84,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "\n",
      "EPOCH:1, MINIBATCH_ID:1/369 RESULTS\n",
      "training loss=2.2974, training accuracy=9.3750%\n",
      "validation loss=2.3031, validation accuracy=11.0000%\n",
      "EPOCH:1, MINIBATCH_ID:2/369 STARTED\n",
      "\n",
      "\ttraining inference...\n",
      "forward propagating=CONV: n_C:6,n_H_prev:28,n_W_prev:28,n_C_prev:1,f:5,stride:1,pad:0,n_H:24,n_W:24\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=POOL: n_H_prev:24,n_W_prev:24,n_C_prev:6,f:2,stride:2,n_H:12,n_W:12,n_C:6\n",
      "forward propagating=CONV: n_C:16,n_H_prev:12,n_W_prev:12,n_C_prev:6,f:5,stride:1,pad:0,n_H:8,n_W:8\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=POOL: n_H_prev:8,n_W_prev:8,n_C_prev:16,f:2,stride:2,n_H:4,n_W:4,n_C:16\n",
      "forward propagating=CONV: n_C:120,n_H_prev:4,n_W_prev:4,n_C_prev:16,f:4,stride:1,pad:0,n_H:1,n_W:1\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=CONV: n_C:84,n_H_prev:1,n_W_prev:1,n_C_prev:120,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "forward propagating=RELU LAYER\n",
      "forward propagating=CONV: n_C:10,n_H_prev:1,n_W_prev:1,n_C_prev:84,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "\ttraining backpropagation...\n",
      "backpropagating=CONV: n_C:10,n_H_prev:1,n_W_prev:1,n_C_prev:84,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "backpropagating=RELU LAYER\n",
      "backpropagating=CONV: n_C:84,n_H_prev:1,n_W_prev:1,n_C_prev:120,f:1,stride:1,pad:0,n_H:1,n_W:1\n",
      "backpropagating=RELU LAYER\n",
      "backpropagating=CONV: n_C:120,n_H_prev:4,n_W_prev:4,n_C_prev:16,f:4,stride:1,pad:0,n_H:1,n_W:1\n",
      "backpropagating=POOL: n_H_prev:8,n_W_prev:8,n_C_prev:16,f:2,stride:2,n_H:4,n_W:4,n_C:16\n",
      "backpropagating=RELU LAYER\n",
      "backpropagating=CONV: n_C:16,n_H_prev:12,n_W_prev:12,n_C_prev:6,f:5,stride:1,pad:0,n_H:8,n_W:8\n",
      "backpropagating=POOL: n_H_prev:24,n_W_prev:24,n_C_prev:6,f:2,stride:2,n_H:12,n_W:12,n_C:6\n",
      "backpropagating=RELU LAYER\n",
      "backpropagating=CONV: n_C:6,n_H_prev:28,n_W_prev:28,n_C_prev:1,f:5,stride:1,pad:0,n_H:24,n_W:24\n",
      "\tvalidation inference...\n",
      "forward propagating=CONV: n_C:6,n_H_prev:28,n_W_prev:28,n_C_prev:1,f:5,stride:1,pad:0,n_H:24,n_W:24\n"
     ]
    }
   ],
   "source": [
    "# LeNet5 architecture.\n",
    "\n",
    "# CONV: n_C, n_H_prev, n_W_prev, n_C_prev, f, stride, pad\n",
    "# CONV: (n_H_prev - f + 2*pad) / stride + 1\n",
    "# POOL: n_H_prev, n_W_prev, n_C_prev, f, stride, mode=\"max\" or \"average\"\n",
    "\n",
    "l1 = (\"CONV\", 6, 28, 28, 1, 5, 1, 0)\n",
    "l1_relu = (\"RELU\",)\n",
    "l2 = (\"POOL\", 24, 24, 6, 2, 2, \"average\")\n",
    "l3 = (\"CONV\", 16, 12, 12, 6, 5, 1, 0)\n",
    "l3_relu = (\"RELU\",)\n",
    "l4 = (\"POOL\", 8, 8, 16, 2, 2, \"average\")\n",
    "l5 = (\"FC\", 120, 4, 4, 16)\n",
    "l5_relu = (\"RELU\",)\n",
    "l6 = (\"FC\", 84, 1, 1, 120)\n",
    "l6_relu = (\"RELU\",)\n",
    "l7 = (\"FC\", 10, 1, 1, 84)\n",
    "\n",
    "LeNet5_architecture = [l1, l1_relu, l2, l3, l3_relu, l4, l5, l5_relu, l6, l6_relu, l7]\n",
    "\n",
    "LeNet5 = ConvolutionalNeuralNetwork(architecture=LeNet5_architecture, compute_validation=True)\n",
    "print(LeNet5)\n",
    "\n",
    "train_start_time = time.time()\n",
    "\n",
    "LeNet5.train(X=x_train, y=y_train, epoch=1, minibatch_size=32, X_val=x_val, y_val=y_val)\n",
    "\n",
    "training_time = time.time() - train_start_time\n",
    "\n",
    "print(\"\\nTraining finished in {0:.2f} seconds\".format(training_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LeNet5' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ed3cc685d888>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprobs_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLeNet5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\Testing info: loss={0:.4f}, accuracy={1:.4f}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_test\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LeNet5' is not defined"
     ]
    }
   ],
   "source": [
    "probs_test, loss_test, predictions_test, accuracy_test = LeNet5.inference(X=x_test, y=y_test)\n",
    "print(\"\\Testing info: loss={0:.4f}, accuracy={1:.4f}%\".format(loss_test, accuracy_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (naive_cnn_env)",
   "language": "python",
   "name": "naive_cnn_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
